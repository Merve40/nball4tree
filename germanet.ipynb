{"cells":[{"cell_type":"markdown","execution_count":1,"metadata":{},"outputs":[],"source":"# German N-Ball Embeddings\n### contributed by [Merve Sahin](https://github.com/Merve40) <br> <br>\n      \nThis notebook is intended for the AI Language Technology lab (Summer Semester 2019) offered at B-IT (Bonn-Aachen International Center for Information Technology). It provides the pre-processing algorithm of german word-senses, which are used as inputs for the n-ball training.\n\n## Table of Contents\n* **[Prerequisite](#Prerequisite)**\n* **[Setup](#setup)**\n* **[The Germanet Package](#the-germanet-package)**\n    * [The Node Class](#the-node-class)\n    * [The Tree Class](#the-tree-class)\n    * [The GermaNet Utility Class](#The-GermaNet-Utility-Class)\n    * [Generate Input Files](#Generate-Input-Files)\n* **[Experiment 1: Training and evaluating nball embeddings](#Experiment-1:-Training-and-evaluating-nball-embeddings)**\n    * [Experiment 1.1: Training nball embeddings](#Experiment-1.1:-Training-nball-embeddings)\n    * [Experiment 1.2: Checking whether tree structures are perfectly embedded into word-embeddings](#Experiment-1.2:-Checking-whether-tree-structures-are-perfectly-embedded-into-word-embeddings)\n* **[Experiment 2: Observe neighbors of word-sense using nball embeddings](#Experiment-2:-Observe-neighbors-of-word-sense-using-nball-embeddings)**\n\n"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Prerequisite\n* Install [Mongo DB](https://docs.mongodb.com/manual/installation/#mongodb-community-edition-installation-tutorials)\n* Run Mongo DB\n\n# Setup\nInstalls **pygermanet**, a python package for querying GermaNet, the german WordNet"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"! pip install pygermanet"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"Downloads the git repository for german n-balls and initializes directories.<br> \n*(Word Embedding file takes up to 4.5 GB space)*"},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"application/json":{"cell":{"!":"OSMagics","HTML":"Other","SVG":"Other","bash":"Other","capture":"ExecutionMagics","debug":"ExecutionMagics","file":"Other","html":"DisplayMagics","javascript":"DisplayMagics","js":"DisplayMagics","latex":"DisplayMagics","markdown":"DisplayMagics","perl":"Other","prun":"ExecutionMagics","pypy":"Other","python":"Other","python2":"Other","python3":"Other","ruby":"Other","script":"ScriptMagics","sh":"Other","svg":"DisplayMagics","sx":"OSMagics","system":"OSMagics","time":"ExecutionMagics","timeit":"ExecutionMagics","writefile":"OSMagics"},"line":{"alias":"OSMagics","alias_magic":"BasicMagics","autocall":"AutoMagics","automagic":"AutoMagics","autosave":"KernelMagics","bookmark":"OSMagics","cat":"Other","cd":"OSMagics","clear":"KernelMagics","colors":"BasicMagics","config":"ConfigMagics","connect_info":"KernelMagics","cp":"Other","debug":"ExecutionMagics","dhist":"OSMagics","dirs":"OSMagics","doctest_mode":"BasicMagics","ed":"Other","edit":"KernelMagics","env":"OSMagics","gui":"BasicMagics","hist":"Other","history":"HistoryMagics","killbgscripts":"ScriptMagics","ldir":"Other","less":"KernelMagics","lf":"Other","lk":"Other","ll":"Other","load":"CodeMagics","load_ext":"ExtensionMagics","loadpy":"CodeMagics","logoff":"LoggingMagics","logon":"LoggingMagics","logstart":"LoggingMagics","logstate":"LoggingMagics","logstop":"LoggingMagics","ls":"Other","lsmagic":"BasicMagics","lx":"Other","macro":"ExecutionMagics","magic":"BasicMagics","man":"KernelMagics","matplotlib":"PylabMagics","mkdir":"Other","more":"KernelMagics","mv":"Other","notebook":"BasicMagics","page":"BasicMagics","pastebin":"CodeMagics","pdb":"ExecutionMagics","pdef":"NamespaceMagics","pdoc":"NamespaceMagics","pfile":"NamespaceMagics","pinfo":"NamespaceMagics","pinfo2":"NamespaceMagics","pip":"BasicMagics","popd":"OSMagics","pprint":"BasicMagics","precision":"BasicMagics","profile":"BasicMagics","prun":"ExecutionMagics","psearch":"NamespaceMagics","psource":"NamespaceMagics","pushd":"OSMagics","pwd":"OSMagics","pycat":"OSMagics","pylab":"PylabMagics","qtconsole":"KernelMagics","quickref":"BasicMagics","recall":"HistoryMagics","rehashx":"OSMagics","reload_ext":"ExtensionMagics","rep":"Other","rerun":"HistoryMagics","reset":"NamespaceMagics","reset_selective":"NamespaceMagics","rm":"Other","rmdir":"Other","run":"ExecutionMagics","save":"CodeMagics","sc":"OSMagics","set_env":"OSMagics","store":"StoreMagics","sx":"OSMagics","system":"OSMagics","tb":"ExecutionMagics","time":"ExecutionMagics","timeit":"ExecutionMagics","unalias":"OSMagics","unload_ext":"ExtensionMagics","who":"NamespaceMagics","who_ls":"NamespaceMagics","whos":"NamespaceMagics","xdel":"NamespaceMagics","xmode":"BasicMagics"}},"text/plain":"Available line magics:\n%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode\n\nAvailable cell magics:\n%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%markdown  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile\n\nAutomagic is ON, % prefix IS NOT needed for line magics."},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":"%%script bash\ngit clone https://github.com/Merve40/nball4tree.git\ncd nball4tree/data/\nunzip GermaNet.zip\nunzip mongo_dump_germanet.zip\nmongorestore --db germanet dump/germanet/\nwget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\ngunzip cc.de.300.vec.gz\nmv cc.de.300.vec w2v.vec \ncd .."},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":""},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# The Germanet Package\nThe following code shows the datastructures and utility classes for constructing a tree of word-senses, as well as a file containing parent-location codes."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"from pygermanet import load_germanet\nfrom xml.sax import make_parser, handler\nimport os"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## The Node Class\nDefines the basis of an **n-ary tree** data structure. The chaining of nodes creates a tree. Each node:\n* has an index, indicating the number/position of the node amongst it's siblings. The index is used later to construct the **parent-location-code** of word-senses. According to the research paper, an index cannot be 0.\n* has a reference to it's parent and children\n* has a word-sense"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"class Node:\n    \n    def __init__(self, word, index=1, parent=None):\n        self.index = index\n        self.word = word\n        self.parent = parent\n        self.children = []\n\n    def add_child(self, word):\n        if self.has_child(word):\n            return None\n\n        index = len(self.children)+1\n        node = Node(word, index, self)\n        self.children.append(node)\n        return node\n\n    def has_child(self, word):\n        return self.get_child(word) is not None\n\n    def get_child(self, word):\n        for child in self.children:\n            if child.word == word:\n                return child\n\n    def is_leaf(self):\n        return not self.children\n    \n    def get_path(self):\n        \"\"\"\n        Retrieves the parent-location-code by tracing back to \n        the root node and recording the index of each parent.\n        \"\"\"\n        indices = []\n        node = self.parent\n        # if current node is root\n        if not node:\n            return ['1']\n\n        while(node is not None):\n            indices.append(str(node.index))\n            node = node.parent\n        return indices[::-1]\n\n    def get_path_string(self, depth=None):\n        path = self.get_path()\n        \n        if depth is not None:\n            length = depth - len(path)\n            path += ['0']*length\n        \n        return self.word + \" \"+\" \".join(path)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## The Tree Class\nInitializes a node with a default root. The tree class provides methods to generate a tree file and a parent-location-code file.<br>\nThe method **add_hypernym_path** adds hypernym paths queried from *pygermanet* one by one to the tree.\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"class Tree:\n    \n    def __init__(self):\n        self.root = Node('*root*')\n        self.depth = 1\n        self.words = set()\n\n    def add_hypernym_path(self, ordered_path, embedded_words, ignore_duplicates):\n        \"\"\"Adds a hypernym path of a word.\n\n        :param ordered_path: ordered list of parents of a word/synset, starting from root\n        :param embedded_words: set containing word-embeddings\n        :param ignore_duplicates: True if duplicate nodes with different hypernym paths should be ignored, else False\n        \"\"\"\n\n        node = self.root\n        current_len = 0\n        for synset in ordered_path[1:]:\n            current_len += 1\n            child = synset.__str__()[7:-1]\n            #avoids nodes with multiple word-compositions\n            if len(child.split()) > 1:\n                break\n            elif child.split('.')[0] not in embedded_words:\n                break\n            elif node.has_child(child):\n                node = node.get_child(child)\n            elif ignore_duplicates and child in self.words:\n                break\n            else:\n                self.words.add(child)\n                node = node.add_child(child)\n            \n        if current_len > self.depth:\n            self.depth = len(ordered_path)\n\n            \n\n    def write_parent_location_code(self, outputfile):\n        \"\"\"Writes parent locations of all words into a file.\n\n        :param outputfile: file to write into\n        \"\"\"\n\n        if os.path.isfile(outputfile):\n            return\n\n        def traverse(node, file):\n            code = node.get_path_string(self.depth)\n            file.write(code+\"\\n\")\n            for child in node.children:\n                traverse(child, file)\n\n        node = self.root\n        with open(outputfile, 'w') as file:\n            traverse(node, file)\n\n\n    def write_tree(self, outputfile):\n        \"\"\"Writes the elements of the tree into a file.\n        The structure of the output follows the breadth-first-search approach.\n\n        :param outputfile: the file to write into\n        \"\"\"\n\n        def traverse(node, visited, file):\n            code = node.get_path_string()\n            if node.is_leaf():\n                if not code in visited:\n                    file.write(node.word+\"\\n\")\n                    visited.add(code)\n            else:\n                if not code in visited:\n                    file.write(node.word +\" \"+\" \".join(map(lambda n: n.word, node.children)))\n                    file.write('\\n')\n                    visited.add(code)\n                for child in node.children:\n                    traverse(child, visited, file)\n\n        visited = {'0'}\n        node = self.root\n        with open(outputfile, 'w') as file:\n            traverse(node, visited, file)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## The GermaNet Utility Class\nThis class scans for XML files containing word-senses, parses them to a **tree** datastructure and loads it into memory."},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"class GermaNetUtil:\n\t__source = ''\n\t__w2vec_file = ''\n\t__new_w2v_file = ''\n\t__ignore_duplicates = True\n\t\n\n\tdef __init__(self, sourcefolder, wordvec_file, new_w2v_file, ignore_duplicates=True):\n\t\t\"\"\"\n\t\t:param sourcefolder: folder containing all xml files from GermaNet\n\t\t:param wordvec_file: file containing german word-embeddings\n\t\t\"\"\"\n\n\t\tself.__source = sourcefolder\n\t\tself.__w2vec_file = wordvec_file\n\t\tself.__new_w2v_file = new_w2v_file\n\t\tself.__ignore_duplicates = ignore_duplicates\n\n\tdef load_tree(self, outputfile):\n\t\t\"\"\"Creates a tree and fills it with words and hypernyms.\n\n\t\t:param outputfile: the outputfile to be created containing all words\n\t\t:return: complete tree\n\t\t\"\"\"\n\n\t\tgermanet = load_germanet()\n\n\t\t# step 1: extract words from GermaNet\n\t\tprint(\"extracting words..\")\n\t\twords, embedded_words = self.__extract_words(germanet, outputfile)\n\t\t\t\t\n\t\t# step 2: fill tree with hypernym paths\n\t\tcount = 0\n\t\tskipped = 0\n\t\tmult_paths = 0\n\t\ttree = Tree()\n\t\tfor word in words:\n\t\t\tsynset = germanet.synset(word) \n\t\t\tif synset is None:\n\t\t\t\tskipped += 1\n\t\t\t\tcontinue\n\t\t\tpaths = synset.hypernym_paths\n\n\t\t\tif len(paths) == 0:\n\t\t\t\tskipped += 1\n\t\t\t\treturn\n\t\t\telif len(paths) > 0 and isinstance(paths[0], list): # checks if synset has multiple paths\n\t\t\t\tmult_paths += 1\n\t\t\t\tfor path in paths:\n\t\t\t\t\tcount +=1\n\t\t\t\t\ttree.add_hypernym_path(path, embedded_words, self.__ignore_duplicates)\n\t\t\telse:\n\t\t\t\tcount+=1\n\t\t\t\ttree.add_hypernym_path(paths, embedded_words, self.__ignore_duplicates)\n\n\t\tprint(\"number of words added = \"+str(len(tree.words)))\n\t\tprint(\"number of paths = \"+str(count))\n\t\tprint(\"number of synsets with multiple paths = \"+str(mult_paths))\n\t\tprint(\"skipped = \"+str(skipped))\n\n\t\treturn tree\n\n\tdef __get_synset_name(self, synset):\n\t\treturn synset.__str__()[7:-1]\n\n\tdef __extract_words(self, germanet, outputFile):\n\t\t\"\"\"Extracts words from xml files.\n\t\tOnly those contained in the word-vector embedding are kept.\n\n\t\t:param outputFile: destination file\n\t\t\"\"\"\n\n\t\tdir = self.__source\n\t\twordVecFile = self.__w2vec_file\n\n\t\t# extracts all existing words from the word-embedding file\n\t\tdef get_vector_words(file):\n\t\t\twords = []\n\t\t\twith open(file, 'r') as f_in:\n\t\t\t\tfor line in f_in:\n\t\t\t\t\twords.append(line.split()[0])\n\t\t\treturn set(words)\n\t\n\n\t\tdef get_words(dir):\n\t\t\tprefixes = ['adj', 'verben', 'nomen']\n\t\t\tfiles = [dir+file for file in os.listdir(dir) if file.split('.')[0] in prefixes]\n\n\t\t\tclass ContentH(handler.ContentHandler):\n\t\t\t\tdef __init__(self):\n\t\t\t\t\tself.words = []\n\t\t\t\t\tself.current_content = \"\"\n\n\t\t\t\tdef startElement(self, name, attrs):\n\t\t\t\t\tself.current_content = \"\"\n\n\t\t\t\tdef characters(self, content):\n\t\t\t\t\tif len(content.strip().split()) == 1:\n\t\t\t\t\t\tself.current_content += content.strip()\n\n\t\t\t\tdef endElement(self, name):\n\t\t\t\t\tif name.lower() == \"orthform\" and self.current_content:\n\t\t\t\t\t\tself.words.append(self.current_content)\n\t\t\t\t\t\t\n\t\t\twords = []\n\t\t\tfor file in files:\n\t\t\t\tparser = make_parser()\n\t\t\t\tcontent_handler = ContentH()\n\t\t\t\tparser.setContentHandler(content_handler)\n\t\t\t\tparser.parse(file)\n\t\t\t\twords += content_handler.words\n\t\t\treturn set(words)\n\n\n\n\t\t# step 1: get all words from word-embedding\n\t\tembedded_words = get_vector_words(wordVecFile)\n\t\tprint(\"number of word embeddings = \"+str(len(embedded_words)))\n\n\t\t# skips step 2 and 3\n\t\tif os.path.isfile(outputFile):\n\t\t\tprint(\"skipping step 2 and 3\")\n\t\t\tsynsets = set()\n\t\t\twith open(outputFile, 'r') as file:\n\t\t\t\tfor line in file:\n\t\t\t\t\tsynsets.add(line[:-1])\n\t\t\t\tprint(\"number of words in word embeddings and GermaNet = \"+str(len(synsets)))\n\t\t\treturn synsets, embedded_words\n\n\t\t# step 2: read words from WordNet\n\t\twords = get_words(dir)\n\t\tprint(\"number of words in GermaNet = \"+str(len(words)))\n\t\t# step 3: discard words that are not present in word-embedding\n\t\tretained_words = words.intersection(embedded_words)\n\t\tprint(\"number of same words in word embeddings and GermaNet = \"+str(len(retained_words)))\n\n\t\tsynsets = []\n\t\twith open(outputFile, 'w') as f:\n\t\t\tfor word in retained_words:\n\t\t\t\tlst = [self.__get_synset_name(ele) for ele in germanet.synsets(word)]\n\t\t\t\tsynsets += lst\n\t\t\t\tf.write(\"\\n\".join(lst))\n\t\t\t\tf.write('\\n')\n\n\t\treturn set(synsets), embedded_words\n\n\tdef create_w2v_file(self, tree):\n\t\t\"\"\"\n\t\tCreates a new word-to-vector file, which contains only those vectors that are also contained in Germanet.\n\n\t\t:param tree: Tree\n\t\t\"\"\"\n\t\twords = set()\n\t\tq = [tree.root]\n\t\twhile len(q) > 0:\n\t\t\tnode = q.pop()\n\t\t\twords.add(node.word.split('.')[0])\n\n\t\t\tfor child in node.children:\n\t\t\t\tq.insert(0, child)\n\n\t\twith open(self.__w2vec_file, 'r') as f, open(self.__new_w2v_file, 'w+') as out:\n\t\t\tfor line in f:\n\t\t\t\tword = line.split()[0]\n\t\t\t\tif word in words:\n\t\t\t\t\tout.write(line)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Generate Input Files\nThe following method makes a call to *GermaNetUtil* in order to load and fill the tree. It takes following arguments:\n* **dir**: directory containing all XML files of GermaNet\n* **file_word_embedding**: the location of the word embedding file ( *data/w2v.vec* )\n* **out_dir**: output directory to generate the files into ( *data/* )"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"def generate_germanet(dir, file_word_embedding, out_dir):\n\tif not out_dir.endswith('/'):\n\t\tout_dir = out_dir + '/'\n\n\toutput_w2v = out_dir+'ws_w2v.vec'\n\toutput_words = out_dir+'ws_words.txt'\n\toutput_tree = out_dir+'ws_child.txt'\n\toutput_codes = out_dir+'ws_catcode.txt'\n\n\tutil = GermaNetUtil(dir, file_word_embedding, output_w2v)\n\n\tprint(\"----------- loading tree -----------------------\")\n\ttree = util.load_tree(output_words)\n\n\tprint(\"\\n----------- writing parent location codes ------\")\n\ttree.write_parent_location_code(output_codes)\n\tprint(\"created \"+output_codes)\n\n\tprint(\"\\n----------- writing tree -----------------------\")\n\ttree.write_tree(output_tree)\n\tprint(\"created \"+output_tree+'\\n')\n\n\twords = []\n\tleafs = []\n\twith open(output_tree, 'r') as f:\n\t\tfor line in f.readlines():\n\t\t\twords.append(line[:-1])\n\t\t\tif len(line[:-1].split()) == 1:\n\t\t\t\tleafs.append(line[:-1])\n\n\tcodes = []\n\twith open(output_codes, 'r') as f:\n\t\tfor line in f.readlines():\n\t\t\tcodes.append(line.split()[0])\n\n\tprint(\"tree:\")\n\tprint(\"nodes=\"+str(len(codes)))\n\tprint(\"leafs=\"+str(len(leafs)))\n\tprint(\"max-depth=\"+str(tree.depth))\n\n\tprint(\"\\n----------- writing new w2v file ---------------\")\n\tutil.create_w2v_file(tree)\n\tprint(\"created \"+output_w2v)\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"xml_path = 'data/GermaNet/'\nword_embedding = 'data/w2v.vec'\noutput_dir = 'data/'\n\ngenerate_germanet(xml_path, word_embedding, output_dir)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Experiment 1: Training and evaluating nball embeddings\n## Experiment 1.1: Training nball embeddings\nThe training can be performed using the generated german input files.<br> \nThe resulting files contain over 50k word-senses.<br> \nThe training can take up to 25 hrs on a normal computer.<br><br>\n\nThe germanet package produces a reduced word embedding file, in order to avoid loading large files into memory. The new word embedding file (*data/ws_w2v.vec*) should be used as input for the training command shown below:\n"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"! touch data/nball.txt\n! python nball.py --train_nball data/nball.txt --w2v data/ws_w2v.vec --ws_child data/ws_child.txt --ws_catcode data/ws_catcode.txt --log data/log.txt &"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Experiment 1.2: Checking whether tree structures are perfectly embedded into word-embeddings"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"! python nball.py --zero_energy data/data_out/ --ball data/nball.txt --ws_child data/ws_child.txt &"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Experiment 2: Observe neighbors of word-sense using nball embeddings"},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":"! python nball.py --neighbors handeln.v.2 handeln.v.5 Teil.n.2 Teil.n.3 Teil.n.4 Teil.n.5 Gerät.n.1 Gerät.n.2 --ball data/nball.txt --num 12"}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}